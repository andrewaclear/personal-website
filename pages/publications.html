
<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Chris J. Maddison</title>
		<link rel="stylesheet" href="../github-markdown.css">
		<style>
			.markdown-body {
				box-sizing: border-box;
				min-width: 200px;
				max-width: 980px;
				margin: 0 auto;
				padding: 45px;
			}

			@media (max-width: 767px) {
				.markdown-body {
					padding: 15px;
				}
			}
			html {
			  width: 100vw;
			}
			body {
			  overflow-x: hidden;
			}
		</style>
	</head>
	<body class="markdown-body">
			<h1>Chris J. Maddison</h1>

			<h2>Publications</h2>
			<ul>
			<li>
				<span style="font-weight:bold">Contrastive Learning Can Find An Optimal Basis For Approximately View-Invariant Functions</span><br>
				<span>Daniel D. Johnson, Ayoub El Hanchi, Chris J. Maddison</span><br>
				<span>ICLR, 2023</span><br>
				<a href="https://arxiv.org/abs/2210.01883">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Stochastic Reweighted Gradient Descent</span><br>
				<span>Ayoub El Hanchi, David Stephens, Chris J. Maddison</span><br>
				<span>ICML, 2022</span><br>
				<a href="https://arxiv.org/abs/2103.12293">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Learning To Cut By Looking Ahead: Cutting Plane Selection via Imitation Learning</span><br>
				<span>Max B Paulus, Giulia Zarpellon, Andreas Krause, Laurent Charlin, Chris J. Maddison</span><br>
				<span>ICML, 2022</span><br>
				<a href="https://arxiv.org/abs/2206.13414">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Augment with Care: Contrastive Learning for the Boolean Satisfiability Problem</span><br>
				<span>Haonan Duan, Pashootan Vaezipoor, Max B. Paulus, Yangjun Ruan, Chris J. Maddison</span><br>
				<span>ICML, 2022</span><br>
				<a href="https://arxiv.org/abs/2202.08396">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Bayesian Nonparametrics for Offline Skill Discovery</span><br>
				<span>Valentin Villecroze, Harry J. Braviner, Panteha Naderian, Chris J. Maddison, Gabriel Loaiza-Ganem</span><br>
				<span>ICML, 2022</span><br>
				<a href="https://arxiv.org/abs/2202.04675">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Optimal Representations for Covariate Shift</span><br>
				<span>Yangjun Ruan*, Yann Dubois*, Chris J. Maddison</span><br>
				<span>ICLR, 2022</span><br>
				<a href="https://arxiv.org/abs/2201.00057">[arXiv]</a>
			</li>
				<li>
					<span style="font-weight:bold">Lossy Compression for Lossless Prediction</span><br>
					<span>Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, Chris J. Maddison</span><br>
					<span>NeurIPS, 2021, <span style="color:red"><i>Spotlight presentation</i></span></span><br>
					<a href="https://arxiv.org/abs/2106.10800">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Learning Generalized Gumbel-max Causal Mechanisms</span><br>
					<span>Guy Lorberbom*, Daniel D. Johnson*, Chris J. Maddison, Daniel Tarlow, Tamir Hazan</span><br>
					<span>NeurIPS, 2021, <span style="color:red"><i>Spotlight presentation</i></span></span><br>
					<a href="https://arxiv.org/abs/2111.06888">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding</span><br>
					<span>Yangjun Ruan*, Karen Ullrich*, Daniel Severo*, James Townsend, Ashish Khisti, Arnaud Doucet, Alireza Makhzani,</span><br>
					<span>Chris J. Maddison</span><br>
					<span>ICML, 2021, <span style="color:red"><i>Long presentation</i></span></span><br>
					<a href="https://arxiv.org/abs/2102.11086">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Oops I Took A Gradient: Scalable Sampling for Discrete Distributions</span><br>
					<span>Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, Chris J. Maddison</span><br>
					<span>ICML, 2021, <span style="color:red"><i>Outstanding Paper Award Honorable Mention</i></span></span><br>
					<a href="https://arxiv.org/abs/2102.04509">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Dual Space Preconditioning for Gradient Descent</span><br>
					<span>Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Arnaud Doucet</span><br>
					<span>SIAM Journal on Optimization, 2021</span><br>
					<a href="https://arxiv.org/abs/1902.02257">[arXiv]</a> <a href="https://epubs.siam.org/doi/abs/10.1137/19M130858X">[SIAM]</a>
				</li>
				<li>
					<span style="font-weight:bold">Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator</span><br>
					<span>Max B. Paulus, Chris J. Maddison, Andreas Krause</span><br>
					<span>ICLR, 2021, <span style="color:red"><i>Oral presentation</i></span></span><br>
					<!-- <span style="color:red">[Oral Presentation]</span><br> -->
					<a href="https://arxiv.org/abs/2010.04838">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Learning Branching Heuristics for Propositional Model Counting</span><br>
					<span>Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Chris J. Maddison, Roger Grosse, Edward Lee, Sanjit A. Seshia, Fahiem Bacchus</span><br>
					<span>AAAI, 2021</span><br>
					<a href="https://arxiv.org/abs/2007.03204">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Gradient Estimation with Stochastic Softmax Tricks</span><br>
					<span>Max B. Paulus*, Dami Choi*, Daniel Tarlow, Andreas Krause, Chris J. Maddison</span><br>
					<span>NeurIPS, 2020, <span style="color:red"><i>Oral presentation</i></span></span><br>
					<!-- <span style="color:red">[Oral Presentation]</span><br> -->
					<a href="https://arxiv.org/abs/2006.08063">[arXiv]</a>
				</li>
				<li>
					<span style="font-weight:bold">Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces</span><br>
					<span>Guy Lorberbom, Chris J. Maddison, Nicolas Heess, Tamir Hazan, Daniel Tarlow</span><br>
					<span>NeurIPS, 2020</span><br>
					<a href="https://arxiv.org/abs/1906.06062">[arXiv]</a>
				</li>
				<li>
			    <span style="font-weight:bold">Hamiltonian descent for composite objectives</span><br>
			    <span>Brendan O'Donoghue, Chris J. Maddison</span><br>
					<span>NeurIPS, 2019</span><br>
			    <a href="https://arxiv.org/abs/1906.02608">[arXiv]</a>
			  </li>
				<li>
					<span style="font-weight:bold">Hierarchical Representations with Poincar&eacute; Variational Auto-Encoders</span><br>
					<span>Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, Yee Whye Teh</span><br>
					<span>NeurIPS, 2019</span><br>
					<a href="https://arxiv.org/abs/1901.06033">[arXiv]</a>
				</li>
			  <li>
			    <span style="font-weight:bold">Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives</span><br>
			    <span>George Tucker, Dieterich Lawson, Shixiang Gu, Chris J. Maddison</span><br>
			    <span>ICLR, 2019</span><br>
			    <a href="https://arxiv.org/abs/1810.04152">[arXiv]</a>
			  </li>
			  <li>
			    <span style="font-weight:bold">Conditional Neural Processes</span><br>
			    <span>Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, S. M. Ali Eslami</span><br>
			    <span>ICML, 2018</span><br>
          <a href="https://arxiv.org/abs/1807.01613" >[arXiv]</a>
			  </li>
			  <li>
			    <span style="font-weight:bold"> Tighter Variational Bounds are Not Necessarily Better</span><br>
			    <span>Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, Yee Whye Teh</span><br>
			    <span>ICML, 2018</span><br>
			    <a href="https://arxiv.org/abs/1802.04537">[arXiv]</a>
				  <a href="./tighter.bib">[bibtex]</a>
			  </li>
			  <li>
			    <span style="font-weight:bold">Filtering Variational Objectives</span><br>
			    <span>Chris J. Maddison*, Dieterich Lawson*, George Tucker*, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh</span><br>
			    <span>NeurIPS, 2017</span><br>
			    <!-- <span style="color:red">[Best Paper Award at the Deep Structured Prediction Workshop ICML 2017]</span><br> -->
			    <a href="https://arxiv.org/abs/1705.09279">[arXiv]</a>
				  <a href="./fivo.bib">[bibtex]</a>
			    </li>
			  <li>
			    <span style="font-weight:bold">REBAR : Low-variance, unbiased gradient estimates for discrete latent variable models</span><br>
			    <span>George Tucker, Andriy Mnih, Chris J. Maddison, Jascha Sohl-Dickstein</span><br>
					<span>NeurIPS, 2017, <span style="color:red"><i>Oral presentation</i></span></span><br>
			    <!-- <span style="color:red">[Oral Presentation]</span><br> -->
			    <a href="https://arxiv.org/abs/1703.07370">[arXiv]</a>
				  <a href="./rebar.bib">[bibtex]</a>
			    </li>
			  <li>
			  <span style="font-weight:bold">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</span><br>
			  <span>Chris J. Maddison, Andriy Mnih, and Yee Whye Teh</span><br>
			  <span>ICLR, 2017</span><br>
			  <a href="https://arxiv.org/abs/1611.00712">[arXiv]</a>
			  <a href="./concrete.bib">[bibtex]</a>
			  </li>
			  <li>
					<span style="font-weight:bold">Mastering the game of Go with deep neural networks and tree search</span> <br>
					<span>David Silver*, Aja Huang*, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis</span> <br>
					<span>Nature, Vol. 529, 484-489, 2016</span> <br>
					<a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[Nature]</a>
          <a href="./alphago.bib">[bibtex]</a>
				  <a href="https://deepmind.com/research/alphago/">[DeepMind AlphaGo]</a>
				</li>
				<li>
					<span style="font-weight:bold">Move Evaluation in Go Using Deep Convolutional Neural Networks</span> <br>
					<span>Chris J. Maddison, Aja Huang, Ilya Sutskever, and David Silver</span> <br>
					<span>ICLR, 2015</span> <br>
					<a href="./deepgo.pdf">[pdf]</a>
					<a href="./deepgo.bib">[bibtex]</a>
					<a href="./deepgogame.sgf">[sgf]</a>
				</li>
				<li>
					<span style="font-weight:bold">A* Sampling</span> <br>
					<span>Chris J. Maddison, Daniel Tarlow, and Tom Minka</span> <br>
					<span>NeurIPS, 2014, <span style="color:red"><i>Outstanding Paper Award</i></span></span><br>
					<!-- <span style="color:red">[Oral Presentation] <b>[Best Paper Award]</b></span><br> -->
					<a href="https://arxiv.org/abs/1411.0030">[arXiv]</a>
					<a href="./astar.bib">[bibtex]</a>
					<a href="./astar.pdf">[pdf]</a>
 					<a href="./astar_supp.pdf">[supplementary]</a>
					<a href="https://github.com/cmaddis/astar-sampling">[code]</a>
				</li>
				<li>
					<span style="font-weight:bold">Structured Generative Models of Natural Source Code</span> <br>
					<span>Chris J. Maddison and Daniel Tarlow</span> <br>
					<span>ICML, 2014</span><br>
					<a href="./nsc.pdf">[pdf]</a>
					<a href="./nsc.bib">[bibtex]</a>
 					<a href="./nsc_supp.pdf">[supplementary]</a>
				</li>
				<li>
					<span style="font-weight:bold">Annealing Between Distributions by Averaging Moments</span> <br>
					<span>Roger Grosse, Chris J. Maddison, and Ruslan Salakhutdinov</span> <br>
					<span>NeurIPS, 2013, <span style="color:red"><i>Oral presentation</i></span></span><br>
					<!-- <span style="color:red">[Oral Presentation]</span><br> -->
					<a href="./aais.pdf">[pdf]</a>
					<a href="./aais.bib">[bibtex]</a>
					<a href="./aaissup.pdf">[supplementary]</a>
					<a href="./aaisrbms_npz.tar.gz">[RBM weights as .npz]</a>
				</li>
				<li>
					<span style="font-weight:bold">Soft song during aggressive interactions: Seasonal changes and endocrine correlates in song sparrows</span> <br>
					<span>Chris J. Maddison, Rindy C. Anderson, Nora H. Prior, Matthew D. Taves, Kiran K. Soma</span> <br>
					<span>Hormones and Behaviour, 2012</span> <br>
					<a href="http://www.sciencedirect.com/science/article/pii/S0018506X12001936">[article]</a>
				</li>
				<li>
					<span style="font-weight:bold">Rapid and Widespread Effects of 17-beta-estradiol on Intracellular Signaling in the Male Songbird Brain</span> <br>
					<span>Sarah A. Heimovics, Nora H. Prior, Chris J. Maddison, Kiran K. Soma</span> <br>
					<span>Endocrinology, 2012</span> <br>
					<a href="http://press.endocrine.org/doi/abs/10.1210/en.2011-1525">[article]</a>
				</li>
			</ul>

			<h2>Book Chapters</h2>
			<ul class="publications">
				<li>
			    <span style="font-weight:bold">Current Interpretability/Explainability Techniques in AI</span><br>
			    <span>Chris J. Maddison</span><br>
			    <span>Responsible AI: A Global Policy Framework. C. Morgan (Ed.), 2019.</span><br>
			  </li>
			  <li>
			    <span style="font-weight:bold">A Poisson process model for Monte Carlo</span><br>
			    <span>Chris J. Maddison</span><br>
			    <span>Perturbation, Optimization, and Statistics. T. Hazan, G. Papandreou, D. Tarlow (Eds.), 2016.</span><br>
			    <a href="./ppmontecarlo.pdf">[pdf]</a>
			    <a href="./ppmontecarlo.bib">[bibtex]</a>
			  </li>
			</ul>

			<h2>Workshop Presentations</h2>
			<ul class="publications">
				<li>
					<span style="font-weight:bold">Twisted Variational Sequential Monte Carlo</span><br>
					<span>Dieterich Lawson, George Tucker, Christian Naesseth, Chris J. Maddison, Ryan Adams, Yee Whye Teh</span><br>
					<span>Bayesian Deep Learning Workshop, NeurIPS, 2018</span><br>
					<a href="./tvsmc.pdf">[pdf]</a>
				</li>
				<li>
			    <span style="font-weight:bold">Particle Value Functions</span><br>
			    <span>Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Arnaud Doucet, Andriy Mnih, Yee Whye Teh</span><br>
			    <span>ICLR Workshop, 2017</span><br>
			    <a href="https://arxiv.org/abs/1703.05820">[arXiv]</a>
		    </li>
			</ul>


		<!-- <h2>Selected Talks</h2>
		<ul class="publications">
			<li>
				<span style="font-weight:bold">Hamiltonian Descent Methods</span><br>
				November 2018, <a href="https://csml.princeton.edu/events/colloquium-hamiltonian-descent-methods" >CSML Colloquium</a>, Princeton University<br>
				<a href="./cmaddison_hamild_slides.pdf" >[slides]</a>
			<li>
				<span style="font-weight:bold">Relaxed Gradient Estimators</span><br>
				March 2018, <a href="http://www.fields.utoronto.ca/activities/17-18/machine-learning" >Machine Learning Advances and Applications Seminar</a>, The Fields Institute<br>
				<a href="./relaxed_grad_estimators_talk.pdf" >[slides]</a>
				<a href="http://www.fields.utoronto.ca/talks/Relaxed-Gradient-Estimators" >[video]</a>
			</li>
			<li>
				<span style="font-weight:bold">A* Sampling</span> <br>
				December 2014, NeurIPS, Montreal, Canada <br>
				<a href="./astartalk.pdf" >[slides]</a>
				<a href="https://www.youtube.com/watch?v=uGlMCxUcGm8" >[video]</a>
			</li>
			<li>
				<span style="font-weight:bold">Annealing Between Distributions by Averaging Moments</span> <br>
				August 2013, University of Cambridge, Cambridge, UK <br>
				<a href="pubs/aaiscamtalk.pdf" >[slides]</a>
			</li>
		</ul> -->
		<h2>Preprints</h2>
		<ul>
			<li>
				<span style="font-weight:bold">On Empirical Comparisons of Optimizers for Deep Learning</span><br>
				<span>Dami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J. Maddison, George E. Dahl</span><br>
				<a href="https://arxiv.org/abs/1910.05446">[arXiv]</a>
			</li>
			<li>
				<span style="font-weight:bold">Hamiltonian Descent Methods</span><br>
				<span>Chris J. Maddison*, Daniel Paulin*, Yee Whye Teh, Brendan O'Donoghue, Arnaud Doucet</span><br>
				<a href="https://arxiv.org/abs/1809.05042">[arXiv]</a>
			</li>
		 </ul>

		<p>* indicates equal contribution.</p>
	</body>
</html>
